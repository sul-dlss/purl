<?xml version="1.0" encoding="UTF-8"?>
<publicObject id="druid:nd387jf5675" published="2023-03-30T18:49:03Z" publishVersion="cocina-models/0.89.1">
  <identityMetadata>
    <objectType>item</objectType>
    <objectLabel>Invariance for perceptual recognition through deep learning</objectLabel>
    <otherId name="catkey">10734942</otherId>
    <otherId name="folio_instance_hrid">a10734942</otherId>
    <sourceId source="sul">dissertationid:0000003674</sourceId>
  </identityMetadata>
  <contentMetadata objectId="druid:nd387jf5675" type="file">
    <resource id="cocina-fileSet-nd387jf5675-main" sequence="1" type="file">
      <label>Body of dissertation</label>
      <file id="phd_thesis_stanford_submission_final-augmented.pdf" mimetype="application/pdf" size="5321987" publish="yes" shelve="yes" preserve="yes">
        <checksum type="sha1">75b4f9d8c77cc7d30d1a075e630a31768f88c281</checksum>
        <checksum type="md5">ada0f104f8ee7fce8e746d38afe6e340</checksum>
      </file>
    </resource>
  </contentMetadata>
  <rightsMetadata>
    <access type="discover">
      <machine>
        <world/>
      </machine>
    </access>
    <access type="read">
      <machine>
        <world/>
      </machine>
    </access>
    <copyright>
      <human>(c) Copyright 2015 by Youzhi Zou</human>
    </copyright>
  </rightsMetadata>
  <rdf:RDF xmlns:fedora="info:fedora/fedora-system:def/relations-external#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
    <rdf:Description rdf:about="info:fedora/druid:nd387jf5675">
    
    
  </rdf:Description>
  </rdf:RDF>
  <oai_dc:dc xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:srw_dc="info:srw/schema/1/dc-schema" xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
    <dc:title>Invariance for perceptual recognition through deep learning</dc:title>
    <dc:contributor>Zou, Youzhi</dc:contributor>
    <dc:contributor>McClelland, James L (primary advisor)</dc:contributor>
    <dc:contributor>Guibas, Leonidas J (advisor)</dc:contributor>
    <dc:contributor>Widrow, Bernard, 1929- (advisor)</dc:contributor>
    <dc:contributor>Stanford University Department of Electrical Engineering.</dc:contributor>
    <dc:type>theses</dc:type>
    <dc:type>Text</dc:type>
    <dc:format>1 online resource.</dc:format>
    <dc:format>electronic</dc:format>
    <dc:language>eng</dc:language>
    <dc:description displayLabel="Abstract">The brain implements recognition systems with incredible competence. Our perceptual systems recognize an object from various perspectives as it transforms through space and time. A key property of effective recognition is invariance to changes in the input. In fact, invariant representations focus on high-level information and neglect irrelevant changes, facilitating effective recognition. It is desirable for computational simulations to capture invariant properties. However, quantifying and designing invariance is difficult, because the input signals to a perceptual system are high dimensional, and the number of input variations, conceived in terms of separate dimensions of variation, such as position, rotation, scale can be exponentially large. Natural invariance resides in a subspace of this exponential space, one that, I argue, can be more effectively captured through learning than through design. To capture perceptual invariance, I take the approach of modeling through deep neural networks. These models are classic AI algorithms. The deep neural network is characteristic of composing simple features from lower layers into more complex representations in higher layers. Going up in the hierarchy, the network forms high-level representations which capture various forms of invariance found in natural images. Within this framework, I present three applications. First, I investigate position-preserving invariance properties of a classical architecture, the convolutional neural network. Indeed, with convolutional networks, I show results surpassing the previous state-of-the-art performance in detecting the location of objects in images. In such models, however, translational invariance is designed, limiting their ability to capture the full invariance structure of real inputs. To learn invariance without design, I exploit unsupervised learning from videos using the 'slowness' principle. Concretely, the unsupervised learning algorithm discovers invariance arising from transformations such as rotation, out-of-plane changes, or warping from motions in video. When quantitatively measured, the learned invariant features are more robust than ones that are hand-crafted. Using such invariant features, recognition in still images is consistently improved. Finally, I explore the development of invariant representations of number through learning from unlabeled examples in a generic neural network. By learning from examples of 'visual numbers', this network forms number representations invariant to object size. With these representations, I illustrate novel simulations for cognitive processes of the 'Approximate Number Sense'. Concretely, I correlate simulations with deep networks with the sensitivity of discrimination across a range of numbers. These simulations capture properties of human number representation, focusing on approximate invariance to other stimulus factors.</dc:description>
    <dc:description type="statement of responsibility">Youzhi (Will) Zou.</dc:description>
    <dc:description>Submitted to the Department of Electrical Engineering.</dc:description>
    <dc:description type="thesis">Thesis (Ph.D.)--Stanford University, 2015.</dc:description>
    <dc:date>2015</dc:date>
    <dc:identifier>https://purl.stanford.edu/nd387jf5675</dc:identifier>
  </oai_dc:dc>
  <mods xmlns="http://www.loc.gov/mods/v3" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:xlink="http://www.w3.org/1999/xlink" version="3.6" xsi:schemaLocation="http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-6.xsd">
    <titleInfo>
      <title>Invariance for perceptual recognition through deep learning</title>
    </titleInfo>
    <name usage="primary" type="personal">
      <namePart>Zou, Youzhi</namePart>
    </name>
    <name type="personal">
      <namePart>McClelland, James L</namePart>
      <role>
        <roleTerm type="text">primary advisor</roleTerm>
      </role>
      <role>
        <roleTerm authority="marcrelator" type="code">ths</roleTerm>
      </role>
    </name>
    <name type="personal">
      <namePart>Guibas, Leonidas J</namePart>
      <role>
        <roleTerm type="text">advisor</roleTerm>
      </role>
      <role>
        <roleTerm authority="marcrelator" type="code">ths</roleTerm>
      </role>
    </name>
    <name type="personal">
      <namePart>Widrow, Bernard</namePart>
      <namePart type="date">1929-</namePart>
      <role>
        <roleTerm type="text">advisor</roleTerm>
      </role>
      <role>
        <roleTerm authority="marcrelator" type="code">ths</roleTerm>
      </role>
    </name>
    <name type="corporate">
      <namePart>Stanford University</namePart>
      <namePart>Department of Electrical Engineering.</namePart>
    </name>
    <genre authority="marcgt">theses</genre>
    <typeOfResource>text</typeOfResource>
    <physicalDescription>
      <form authority="marcform">electronic</form>
      <form authority="gmd">electronic resource</form>
      <form authority="marccategory">electronic resource</form>
      <form authority="marcsmd">remote</form>
      <extent>1 online resource.</extent>
    </physicalDescription>
    <language>
      <languageTerm authority="iso639-2b" type="code">eng</languageTerm>
    </language>
    <abstract displayLabel="Abstract">The brain implements recognition systems with incredible competence. Our perceptual systems recognize an object from various perspectives as it transforms through space and time. A key property of effective recognition is invariance to changes in the input. In fact, invariant representations focus on high-level information and neglect irrelevant changes, facilitating effective recognition. It is desirable for computational simulations to capture invariant properties. However, quantifying and designing invariance is difficult, because the input signals to a perceptual system are high dimensional, and the number of input variations, conceived in terms of separate dimensions of variation, such as position, rotation, scale can be exponentially large. Natural invariance resides in a subspace of this exponential space, one that, I argue, can be more effectively captured through learning than through design. To capture perceptual invariance, I take the approach of modeling through deep neural networks. These models are classic AI algorithms. The deep neural network is characteristic of composing simple features from lower layers into more complex representations in higher layers. Going up in the hierarchy, the network forms high-level representations which capture various forms of invariance found in natural images. Within this framework, I present three applications. First, I investigate position-preserving invariance properties of a classical architecture, the convolutional neural network. Indeed, with convolutional networks, I show results surpassing the previous state-of-the-art performance in detecting the location of objects in images. In such models, however, translational invariance is designed, limiting their ability to capture the full invariance structure of real inputs. To learn invariance without design, I exploit unsupervised learning from videos using the 'slowness' principle. Concretely, the unsupervised learning algorithm discovers invariance arising from transformations such as rotation, out-of-plane changes, or warping from motions in video. When quantitatively measured, the learned invariant features are more robust than ones that are hand-crafted. Using such invariant features, recognition in still images is consistently improved. Finally, I explore the development of invariant representations of number through learning from unlabeled examples in a generic neural network. By learning from examples of 'visual numbers', this network forms number representations invariant to object size. With these representations, I illustrate novel simulations for cognitive processes of the 'Approximate Number Sense'. Concretely, I correlate simulations with deep networks with the sensitivity of discrimination across a range of numbers. These simulations capture properties of human number representation, focusing on approximate invariance to other stimulus factors.</abstract>
    <note type="statement of responsibility">Youzhi (Will) Zou.</note>
    <note>Submitted to the Department of Electrical Engineering.</note>
    <note type="thesis">Thesis (Ph.D.)--Stanford University, 2015.</note>
    <originInfo>
      <dateIssued>2015</dateIssued>
      <place>
        <placeTerm authority="marccountry" type="code">xx</placeTerm>
      </place>
      <issuance>monographic</issuance>
    </originInfo>
    <location>
      <url usage="primary display" displayLabel="electronic resource">https://purl.stanford.edu/nd387jf5675</url>
    </location>
    <recordInfo>
      <recordContentSource authority="marcorg">CSt</recordContentSource>
      <recordOrigin>Converted from MARCXML to MODS version 3.6 using MARC21slim2MODS3-6_SDR.xsl (SUL version 1 2018/06/13; LC Revision 1.118 2018/01/31)</recordOrigin>
      <recordCreationDate encoding="marc">150122</recordCreationDate>
      <recordChangeDate encoding="iso8601">20150131012329.0</recordChangeDate>
      <recordIdentifier source="SIRSI">a10734942</recordIdentifier>
    </recordInfo>
    <accessCondition type="copyright">(c) Copyright 2015 by Youzhi Zou</accessCondition>
    <accessCondition type="license" xlink:href="https://creativecommons.org/licenses/by-nc/3.0/legalcode">This work is licensed under a Creative Commons Attribution Non Commercial 3.0 Unported license (CC BY-NC).</accessCondition>
  </mods>
  <releaseData>
    <release to="Searchworks">true</release>
  </releaseData>
</publicObject>
